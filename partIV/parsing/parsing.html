<h1>Training the Berkeley Parser on the WSJ Penn treebank for the English language</h1>
<h2>What is the Parsing?</h2>
<ul>
<li><strong>INPUT:</strong></li>
</ul>
<p><span style="margin: 0 5em">Boeing is located in Seattle.</pre></p>
<ul>
<li><strong>OUTPUT</strong></li>
</ul>
<p><img style="margin: 0 3em" src="img/boeing_is_located_in_seattle.png" width="300px" height="250px"></img></p>
<p>In the parsing problem we're given a sentence in input and we produce an object called a parse tree.</p>
<p>A <strong>parse tree</strong> is a tree structure with the words in the sentences as leaves of the tree, e.g. <em>Boeing</em>, <em>is</em>, <em>located</em>, <em>in</em>, <em>Seattle</em>. and other labels, such as <em>NP</em>, <em>PP</em>, <em>VP</em>, <em>S</em>, etc., in the inner nodes.</p>
<p>Thus, at very high level we have a hierarchical decomposition of the sentence into the tree structure.</p>
<h2>Unzip the tar.gz archive</h2>
<p><code>$ tar -zxvf berkeley.tar.gz</code></p>
<h2>Probabilistic Context Free Grammar (PCFG)</h2>
<p>Training a parser consists in learning a PCFG generating all the parse trees in a treebank.</p>
<p>A context-free grammar (CFG) is a 4-tuple G = (N, &Sigma;, R, S) where:
<ul>
<li> N is a finite set of non-terminal symbols. </li>
<li> &Sigma; is a finite set of terminal symbols. </li>
<li> R is a finite set of rules of the form X &rarr; Y<sub>1</sub>Y<sub>2</sub> . . . Y<sub>n</sub>, where X &isin; N, n &ge; 0, <br/>
 and Y<sub>i</sub> &isin; (N &cup; &Sigma;) for i = 1 . . . n. </li>
<li> S &isin; N is a distinguished start symbol </li>
</ul></p>
<p>A Pobabilistic context-free grammar (PCFG) is defined as follows:</p>
<p><strong>Definition</strong> A PCFG is a 5-tuple G = (N, &Sigma;, R, S, P), where:</p>
<ol>
<li>
<p>N, &Sigma;, R, S are the same as in a CFG</p>
</li>
<li>
<p>P is the set of probabilities on production rules</p>
</li>
</ol>
<p><img src="img/PCFG.png" width="50%"></img></p>
<p>You can generate a parse tree by repeatedly sampling a rule from a PCFG until you have only terminals.</p>
<h2>Train the Berkeley Parser</h2>
<p>We would like to learn a grammar that generate all possible sentences for the English language.</p>
<p>In order to learn a grammar we need a treebank. A treebank is a corpus of sentences annotated with their corresponding parsed trees. </p>
<p>The treebank for the English language is the Walll Street Journal Penn Tree Bank (PTB).</p>
<p>The file <code>wsj_02-21.parse</code> in the data dir contains all the data from section 02 to section 21 of thte PTB corpus. </p>
<p>To train a parser model for Englis, type the following command:</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.GrammarTrainer -v -SMcycles 1 -path data/wsj_02-21.parse -out wsj_grammar.gr -treebank SINGLEFILE</code></p>
<h2>View a grammar file</h2>
<p>The Berkeley parser reads and writes grammar files as serialized java classes. To view the grammars, you can export them to text format with:</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu/berkeley/nlp/PCFGLA/WriteGrammarToTextFile wsj_grammar.gr wsj_grammar</code></p>
<p>The exported grammar contains the following files:</p>
<ul>
<li><strong>wsj_grammar.lexicon</strong>: Probabilities of rules generating terminal symbol</li>
<li><strong>wsj_grammar.grammar</strong>: Probabilities of rules generating nonterminal symbols</li>
<li><strong>wsj_grammar.words</strong>: list of words appearing in the treebank</li>
</ul>
<h2>Parse with the Berkeley Parser</h2>
<p>Parse 23th fold</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.BerkeleyParser -gr wsj_grammar.gr -inputFile data/wsj_23.pos -useGoldPOS &gt; wsj_23.out</code></p>
<p>Parse 24th fold</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.BerkeleyParser -gr wsj_grammar.gr -inputFile data/wsj_24.pos -useGoldPOS &gt; wsj_24.out</code></p>
<h2>Evaluate the trained model</h2>
<p>Parsers are evaluated according to Precision, Recall anf FMeasure of generated parse trees:</p>
<ul>
<li>
<p>Precision = number of correct constituents in parser output /  number of constituents in the parser output</p>
</li>
<li>
<p>Recall =  number of correct constituens in parser outut / number of constituents in the gold standard</p>
</li>
<li>
<p>F1-Measure = 2 * P * R / (P + R)</p>
</li>
</ul>
<p>Type the following command to evaluate the performance of the parsers on the sentences in the 23th fold:</p>
<p><code>$ ./EVALB/evalb -p EVALB/COLLINS.prm data/wsj_23.gold wsj_23.out</code>
<pre style="width:100%">
Precision | Recall | FMeasure 
--------- | ------ | --------
  72.83%  | 74.61% |  73.71%
</pre></p>
<p>Type the following command to evaluate the performance of the parsers on the sentences in the 24th fold:</p>
<p><code>$ ./EVALB/evalb -p EVALB/COLLINS.prm data/wsj_24.gold wsj_24.out</code>
<pre style="width:100%">
Precision | Recall | FMeasure 
--------- | ------ | --------
  72.01%  | 73.55% |  72.77%
</pre></p>
<p>So, for example, let assume your have the following gold tree (left) and the parse tree (right) for the sentence: <code>This feels more like a one - shot deal.</code></p>
<pre>
 <table>
  <tr>  
   <th style="font-size: 110%; font-family: Arial, Helvetica, sans-serif">1. Gold tree</th>
   <th style="font-size: 110%; font-family: Arial, Helvetica, sans-serif"">2. Parsed tree</th>  
  </tr>
  <tr>
   <td><img src="img/eval_gold_tree80p.png" /></td>
   <td><img src="img/eval_parse_tree80p.png" /></td>
  </tr>
  <tr>
   <td>TOP -> S</td>
   <td style="color: red">TOP -> S</td> 
  </tr>
 <tr>
   <td>S -> `` NP VP .</td>
   <td style="color: red">S -> `` NP VP .</td> 
  </tr>
  <tr>
   <td>NP -> DT </td>
   <td style="color: red">NP -> DT </td>
  </tr>
  <tr>
   <td>VP -> VBZ PP</td> 
   <td style="color: red">VP -> VBZ ADVP</td>
   </td>
  </tr>
  <tr>
   <td>PP -> ADVP IN NP</td>
   <td sytle="color: red">ADVP -> ADV PP</td>
  </tr>
  <tr>
   <td>ADVP -> RBR</td>
   <td style="color: red">ADVP -> RBR</td>
  </tr>
  <tr>
   <td>NP -> DT NML NN</td>
   <td>PP -> IN NP</td>
  </tr>
  <tr>
   <td>NML -> CD HYPH NN</td>
   <td style="color: red">NP -> DT NML NN</td>
  </tr>
  <tr>
   <td></td>
   <td style="color: red">NML -> CD HYPH NN</td>
  </tr>
 </table>
</pre>

<p>Thus:</p>
<p><ul>
  <li> <span style="font-weight:bold"> Precision </span> = <span style="color:red">7</span> / 9 = 0.777 </li>
  <li> <span style="font-weight:bold"> Recall </span> = <span style="color:red">7 </span> / 8 = 0.875 </li>
  <li> <span style="font-weight:bold"> F1-Measure </span> = 2 * 0.777 * 0.875 / (0.777 + 0.875) = 0.823...</li>
 </ul></p>
<pre style="display: none">
```
 $ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.GrammarTrainer -v -SMcycles 2 -path data/wsj_02-21.parse -out wsj_grammar.gr -treebank SINGLEFILE
```

## Train the Berkeley Parser on toy grammar
```
 $ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.GrammarTrainer -v -SMcycles 1 -path toy.parse -out toy.gr -treebank SINGLEFILE
```

## View toy grammar file
```
 $ java -cp libs/BerkeleyParser-1.7.jar edu/berkeley/nlp/PCFGLA/WriteGrammarToTextFile toy.gr toy
```
</pre>

<h2>Parse a sentence input from STDIN</h2>
<p>By default, the BerkeleyParser will read a sentence from STDIN (one per line) and write parse trees to STDOUT.</p>
<p>To read a sentence from STDIN, type the following command:</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.BerkeleyParser -gr wsj_grammar.gr</code></p>
<p>Then, input the following sentence:</p>
<p><code>The dog laughs .</code></p>
<p>Your output should looks like this:</p>
<p><code>( (S (NP (DT The) (NN dog)) (VP (VB laugh)) (. .)) )</code></p>
<p><img alt="parse" src="img/Thedoglaug1610954013.png" /></p>
<p>In order to print a sentence into a file, you have to pass the <em>-outputFile</em> option</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.BerkeleyParser -gr wsj_grammar.gr -outputFile parse.out</code></p>
<p>Then, type <code>CTLR^D</code> to stop the parser execution.</p>
<h2>See the viterbi derivation of a tree.</h2>
<p>A tool for annotating parse trees with their most likely Viterbi derivation over refined categories and scoring the subtrees can be started with:</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA/TreeLabeler -gr wsj_grammar.gr</code></p>
<p>Then, type:</p>
<p><code>(TOP (S (NP (DT The) (NN dog)) (VP (VB laugh)) (. .)) )</code></p>
<p>The output should look something like this:</p>
<p><code>( (S-0 (@S-1 (NP-2 (DT-2 The) (NN-1 dog)) (VP-1 (VB-1 laugh))) (.-1 .)))</code></p>
<h2>Compute the log-likelihood of a parse tree.</h2>
<p>This tool reads in parse trees and compute (log-)likelihood of a parse tree, i.e. <strong>T(t|s)</strong>. </p>
<p>To do so, type the following command:</p>
<p><code>$ java -cp libs/BerkeleyParser-1.7.jar edu.berkeley.nlp.PCFGLA.TreeScorer -gr wsj_grammar.gr</code></p>
<p>Then, input the following tree:</p>
<p><code>$ (TOP (S (NP (DT The) (NN dog)) (VP (VB laugh)) (. .)) )</code></p>
<p>You should get the following (log-)likelihood value:
<code>-31.23685977325989</code></p>
<h2>Exercise 1.</h2>
<p>Train for more iterations by incresing the value of the <em>-SMcycles</em> parameter.
Then, parse file <code>wsj_23.pos</code> and <code>wsj_24.pos</code> again. What happens?</p>
<pre style="display: none">
## CKY - A dynamic programming algorithm
* Given a PCFG and a sentence *s*, how do we find 
      $alpha$

</pre>