{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dataset**: Movie Reviews (sentiment analysis).\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from data import load\n",
    "\n",
    "positive_reviews = load('pos')\n",
    "negative_reviews = load('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" america's sweethearts \" has an intriguing premise and a great cast , but it isn't nearly as edgy or funny as it should be . \n",
      "almost all the problems with the project can be traced back to co-script writer billy crystal , who shows the same lack of discipline with the screenplay that he typically displays while co-hosting \" comic relief \" charity shows with robin williams and whoopi goldberg ( two other paragons of self-indulgence ) . \n",
      "crystal ignores a simple , but crucial , rule : for a screwball comedy to work , the characters must be placed into a rigid social setting , because only in that context will their unorthodox antics be humorous . \n",
      " \" america's sweethearts \" takes place at a press junket , where decorum must be maintained in front of the reporters . \n",
      "it's a promising set-up , but the screenplay quickly blows off the rules , thus dissipating the tension of the situation . \n",
      "by the end of the film , all the lead performers participate in a huge fight with a room full of journalists looking on , but their outbursts are only mildly amusing because the structure has been destroyed . \n",
      "john cusack and catherine zeta-jones play eddie thomas and gwen harrison , a beloved acting duo whose marriage hit the skids when gwen began seeing hector ( hank azaria ) , a spanish actor with an ego almost as pronounced as his lisp . \n",
      "of the last nine films eddie and gwen made together , six crossed the $100 million mark , but the prospects for their final effort , a space opus titled \" time over time , \" are far from rosy . \n",
      "while eddie has spent many months in a new age rest clinic fretting over the breakup , gwen's solo films have tanked . \n",
      "to make matter worse , the director of the movie ( christopher walken ) , a \" visionary \" who purchased the unabomber's cabin and had it moved to his backyard , is withholding the film from the studio , insisting that the first screening be held at the junket . \n",
      "desperate to win over the press , the studio elects to hire lee ( billy crystal ) , a recently fired publicist , to salvage the situation . \n",
      "lee hopes to turn lemons into lemonade by convincing eddie and gwen to pretend to be on the road to reconciliation . \n",
      "he enlists the help of kiki ( julia roberts ) , gwen's sister , personal assistant and whipping girl . \n",
      "what lee doesn't know is that kiki is in love with eddie , a fact that could temper her effectiveness . \n",
      "press junkets are a haven for control freaks . \n",
      "studios fly journalists in from around the world and put them up in a plush hotel , with food and drink always at hand . \n",
      "generally , on the evening of their arrival , writers are bussed to see the featured film , then ferreted straight back to the hotel . \n",
      "the next day , writers go to the studio suites and assemble in groups of five or six for roundtable interviews . \n",
      "every 30 minutes or so , a producer , director , writer or actor is brought into the room for a few minutes of questions , with a publicist hovering in the corner to keep an eye on things . \n",
      "the atmosphere is one of cordial oppression ? writers are free to ask what they want , but understand that if the studio dislikes a question , they may not be invited to future junkets . \n",
      "representatives from tv stations face even more restrictions . \n",
      "they get roughly five minutes to interview each member of the cast and crew , with the studio filming the exchanges . \n",
      "the \" reporters \" are notorious for tossing softball questions as they suck up to the stars , but to play it safe , the studios stand ready to erase the tapes if anything unpleasant occurs . \n",
      "placing two spoiled actors in a setting where image is everything is inspired , but the screenplay undermines the conceit . \n",
      "the junket is moved from the handsome , but highly confining , four seasons hotel to a plush resort near las vegas . \n",
      "for most of the film , the movie stars run around the sprawling grounds , completely safe from the eyes of the press . \n",
      "when they do deal with journalists , the \" it is imperative that you be on your best behavior in front of the reporters \" premise is de-clawed . \n",
      "gwen and eddie insult each other while the tv cameras roll , they scream at each other in a restaurant filled with the media and , at the screening of the movie , everyone connected with the film goes nuts , all without any repercussions . \n",
      "lee certainly isn't bothered by any of the infantile outbursts ; in fact , he makes arrangements for footage of even more inappropriate behavior to be delivered to the tabloids . \n",
      "is the studio angry about his handling of the combative actors ? \n",
      "hell no ? they feel lee is a genius for garnering so much publicity for the movie . \n",
      "all of which underscores how billy crystal and co-writer peter tolan screwed up their own premise : the comedy in \" america's sweethearts \" is based on barely-in-control people trying to contain themselves in the presence of reporters , except that it doesn't matter because any publicity is good publicity . \n",
      "and thus the very set-up for the film implodes , leaving smoke and dust in place of laughter . \n",
      "so what about the cast ? \n",
      "julia roberts , at her best playing the underdog , is utterly charming here , although i could have lived without flashbacks that exist solely as an excuse to show her in a fat suit ( and not a very convincing one , by the way ) . \n",
      "catherine zeta-jones makes a believable brat and john cusack fleshes out his obsessed character enough to make him vaguely sympathetic . \n",
      "by casting himself as the publicist , billy crystal allows himself to do roughly the same thing he does on \" comic relief \" - stay on the sidelines of the action while tossing off cornball jokes and snarky remarks . \n",
      "in supporting roles , hank azaria wears out his welcome fast with broad gestures and a spanish accent that speedy gonzales would have deemed \" too broad . \" \n",
      "seth green is amusing as a toadie , stanley tucci is very good as a ruthless studio head and christopher walken plays the eccentric director with suitable flair , though he has little to work with . \n",
      "come to think of it , \" little to work with \" is the operative phrase for this movie . \n",
      "as a hollywood satire , \" america's sweethearts \" is toothless . \n",
      "as a romance , it is at best a minor pleasure . \n",
      "such a good cast , such a waste of their efforts . \n",
      "had it not been taken long ago , a better title for the film would have been \" much ado about nothing . \" \n"
     ]
    }
   ],
   "source": [
    "print(negative_reviews[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "we perform a simple tokenization step using a regex tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def lowercased(reviews):\n",
    "    pattern = re.compile('[\\W]+', re.UNICODE)\n",
    "    for doc in reviews:\n",
    "        yield pattern.sub(r' ', str(doc).replace('\\n', ' ').lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parsed_pos = lowercased(positive_reviews)\n",
    "parsed_neg = lowercased(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The extracted information will be added to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframe(pos, neg):\n",
    "    import pandas as pd\n",
    "    docs = []\n",
    "    for doc in pos:\n",
    "        docs.append([doc, 1])\n",
    "    for doc in neg:\n",
    "        docs.append([doc, 0])\n",
    "    return pd.DataFrame(docs, columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "careful that the function lowercased is a generator and not a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = get_dataframe(parsed_pos, parsed_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes a movie comes along that falls somew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i swear i have seen the edge before in fact it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with the abundance of trite recycled movies in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contact is a film that tries to do several dif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expectation rating a bit worse than expected m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  sometimes a movie comes along that falls somew...      1\n",
       "1  i swear i have seen the edge before in fact it...      1\n",
       "2  with the abundance of trite recycled movies in...      1\n",
       "3  contact is a film that tries to do several dif...      1\n",
       "4  expectation rating a bit worse than expected m...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "To initialize the network we to get a vocabulary in advance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    vocab = set()\n",
    "    for doc in df['text']:\n",
    "        vocab |= set(doc.split())\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39696"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(dataset)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To perform a more efficient indexing of the embeddings we map each word to a unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10037"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2id[word] = i\n",
    "word2id['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Additionally we need to add two special tokens UNK and PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = {'PAD':0, 'UNK':1}\n",
    "for i, word in enumerate(vocab, 2):\n",
    "    word2id[word] = i\n",
    "word2id['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_w2id(vocab):\n",
    "    w2id = {'PAD':0, 'UNK':1}\n",
    "    for i, word in enumerate(vocab, 2):\n",
    "        w2id[word] = i\n",
    "    return w2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39698"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab =  get_vocab(dataset)\n",
    "word2id = create_w2id(vocab)\n",
    "\n",
    "len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally we create a function to convert a document to a vector of unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def doc2ids(doc, word2id):\n",
    "    return list(map(lambda x: word2id.get(x, word2id['UNK']), doc.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And convert our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset['X'] = dataset['text'].apply(lambda x :doc2ids(x, word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes a movie comes along that falls somew...</td>\n",
       "      <td>1</td>\n",
       "      <td>[18709, 6119, 4976, 19592, 13979, 21782, 15443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i swear i have seen the edge before in fact it...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1943, 16863, 1943, 14122, 8610, 10039, 37531,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with the abundance of trite recycled movies in...</td>\n",
       "      <td>1</td>\n",
       "      <td>[4362, 10039, 29077, 28356, 20400, 19701, 6340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contact is a film that tries to do several dif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[38731, 21804, 6119, 6444, 21782, 22489, 3285,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expectation rating a bit worse than expected m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[14067, 37634, 6119, 22692, 29894, 35667, 4642...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  sometimes a movie comes along that falls somew...      1   \n",
       "1  i swear i have seen the edge before in fact it...      1   \n",
       "2  with the abundance of trite recycled movies in...      1   \n",
       "3  contact is a film that tries to do several dif...      1   \n",
       "4  expectation rating a bit worse than expected m...      1   \n",
       "\n",
       "                                                   X  \n",
       "0  [18709, 6119, 4976, 19592, 13979, 21782, 15443...  \n",
       "1  [1943, 16863, 1943, 14122, 8610, 10039, 37531,...  \n",
       "2  [4362, 10039, 29077, 28356, 20400, 19701, 6340...  \n",
       "3  [38731, 21804, 6119, 6444, 21782, 22489, 3285,...  \n",
       "4  [14067, 37634, 6119, 22692, 29894, 35667, 4642...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The network\n",
    "\n",
    "First of all we create train and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(dataset['X'], dataset['label'], test_size=0.2, random_state=42, stratify=dataset['label'])\n",
    "df_X_train, df_X_valid, df_y_train, df_y_valid = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=42, stratify=df_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first network that we are going to define perform the average of word embeddings and learns a logistic regression:\n",
    "\n",
    "- **Embedding layer** (mapping the list of ids to an embedding matrix)\n",
    "- **Global Average Pooling** (we perform the mean of word embedding over the rows)\n",
    "- A **logistic function** at the output (A linear layer followed by a sigmoid non linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class AVGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, nb_emb):\n",
    "        super(AVGNet, self).__init__()\n",
    "        self.emb = nn.Embedding(nb_emb, emb_dim)\n",
    "        self.out = nn.Linear(emb_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embs = self.emb(x)\n",
    "        pool = embs.mean(dim=1)\n",
    "        out  = self.out(pool)\n",
    "        out  = F.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 5)\n",
      "  (out): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mdl = AVGNet(5, len(word2id))\n",
    "print(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sent = 'the film is horrible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_sent = torch.LongTensor([doc2ids(sent, word2id)])\n",
    "fake_sent.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8750,  0.0296,  0.2891, -0.3777,  0.3439],\n",
      "         [-0.0120, -0.9149,  0.2452,  1.5830, -0.7774],\n",
      "         [-0.0546,  0.0819, -0.8250,  1.1426,  0.8698],\n",
      "         [-0.0499, -0.1935, -0.5581, -0.8661,  0.8754]]])\n",
      "tensor([[-0.2479, -0.2492, -0.2122,  0.3705,  0.3279]])\n",
      "tensor([[-0.3732]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4078]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl(fake_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    def pad(sent, max_len):\n",
    "        diff = max_len - len(sent)\n",
    "        return ([0]*diff)+sent\n",
    "    max_len = max(len(sent) for sent in batch)\n",
    "    return list(map(lambda x: pad(x, max_len), batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dataset(X, Y, batch_size=32):\n",
    "    b_x, b_y = [], []\n",
    "    for i, (x, y) in enumerate(zip(X, Y)):\n",
    "        if i%batch_size == batch_size-1:\n",
    "            yield torch.LongTensor(pad_batch(b_x)), torch.FloatTensor(b_y)\n",
    "            b_x, b_y = [], []\n",
    "        b_x.append(x)\n",
    "        b_y.append([y])\n",
    "    yield torch.LongTensor(pad_batch(b_x)), torch.FloatTensor(b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (out): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "0.761\n",
      "0.720\n",
      "0.605\n",
      "0.462\n",
      "0.325\n",
      "0.214\n",
      "0.134\n",
      "0.110\n",
      "0.062\n",
      "0.050\n",
      "81.49 sec\n"
     ]
    }
   ],
   "source": [
    "mdl = AVGNet(300, len(word2id))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.01)\n",
    "print(mdl)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x)\n",
    "        loss = criterion(out, y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{:.3f}'.format(running_loss/i))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (out): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "0.769\n",
      "0.719\n",
      "0.600\n",
      "0.472\n",
      "0.339\n",
      "0.225\n",
      "0.140\n",
      "0.111\n",
      "0.066\n",
      "0.051\n",
      "10.16 sec\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mdl = AVGNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.01)\n",
    "print(mdl)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{:.3f}'.format(running_loss/i))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-04 *\n",
       "       [[ 3.4061]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl(fake_sent.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mdl(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.45975126e-02],\n",
       "       [2.01893374e-02],\n",
       "       [9.89087462e-01],\n",
       "       [8.32840621e-01],\n",
       "       [7.72233009e-01],\n",
       "       [9.94890809e-01],\n",
       "       [7.89847136e-01],\n",
       "       [3.60090025e-02],\n",
       "       [2.04081565e-01],\n",
       "       [9.88839447e-01],\n",
       "       [2.20005840e-01],\n",
       "       [8.73324990e-01],\n",
       "       [4.56796378e-01],\n",
       "       [2.97514498e-01],\n",
       "       [3.45307216e-02],\n",
       "       [1.05342567e-01],\n",
       "       [2.09574178e-01],\n",
       "       [9.10596550e-02],\n",
       "       [9.94313598e-01],\n",
       "       [8.43070924e-01],\n",
       "       [9.10653412e-01],\n",
       "       [3.88665423e-02],\n",
       "       [7.71574676e-01],\n",
       "       [9.06597972e-01],\n",
       "       [4.43765432e-01],\n",
       "       [2.90626466e-01],\n",
       "       [9.29496288e-01],\n",
       "       [9.05572116e-01],\n",
       "       [2.24347413e-01],\n",
       "       [9.89174962e-01],\n",
       "       [2.31569320e-01],\n",
       "       [8.40184152e-01],\n",
       "       [3.78746510e-01],\n",
       "       [1.37483940e-01],\n",
       "       [2.35652570e-02],\n",
       "       [9.73054886e-01],\n",
       "       [9.76708889e-01],\n",
       "       [9.95333850e-01],\n",
       "       [7.86099769e-03],\n",
       "       [7.91313887e-01],\n",
       "       [6.15685761e-01],\n",
       "       [8.23135115e-03],\n",
       "       [2.61285305e-01],\n",
       "       [8.68882000e-01],\n",
       "       [2.74091899e-01],\n",
       "       [8.38864505e-01],\n",
       "       [9.41261828e-01],\n",
       "       [3.78224701e-01],\n",
       "       [9.28517818e-01],\n",
       "       [1.38684407e-01],\n",
       "       [3.23551893e-01],\n",
       "       [6.89870477e-01],\n",
       "       [4.33988571e-02],\n",
       "       [1.36022925e-01],\n",
       "       [8.53814065e-01],\n",
       "       [4.70164746e-01],\n",
       "       [6.47219941e-02],\n",
       "       [9.59613919e-01],\n",
       "       [6.85766518e-01],\n",
       "       [1.56910792e-01],\n",
       "       [5.24163008e-01],\n",
       "       [5.84645212e-01],\n",
       "       [1.03701867e-01],\n",
       "       [2.76939392e-01],\n",
       "       [1.06912352e-01],\n",
       "       [4.29236054e-01],\n",
       "       [8.41949999e-01],\n",
       "       [3.08815211e-01],\n",
       "       [3.14547479e-01],\n",
       "       [1.37851834e-01],\n",
       "       [2.73598164e-01],\n",
       "       [8.58896077e-01],\n",
       "       [9.81800795e-01],\n",
       "       [2.30347767e-01],\n",
       "       [9.97713447e-01],\n",
       "       [1.78937599e-01],\n",
       "       [3.29247653e-01],\n",
       "       [7.97840301e-04],\n",
       "       [9.71654356e-01],\n",
       "       [4.91239369e-01],\n",
       "       [9.41477537e-01],\n",
       "       [9.96950984e-01],\n",
       "       [7.40072668e-01],\n",
       "       [8.85508418e-01],\n",
       "       [7.80476213e-01],\n",
       "       [9.31315362e-01],\n",
       "       [7.67544270e-01],\n",
       "       [4.68083948e-01],\n",
       "       [1.12495869e-01],\n",
       "       [3.23621988e-01],\n",
       "       [5.18059969e-01],\n",
       "       [4.69740123e-01],\n",
       "       [9.40927982e-01],\n",
       "       [1.35460332e-01],\n",
       "       [4.53428209e-01],\n",
       "       [1.79228336e-02],\n",
       "       [1.03346318e-01],\n",
       "       [3.79338712e-01],\n",
       "       [2.51642261e-02],\n",
       "       [3.67964745e-01],\n",
       "       [1.27321526e-01],\n",
       "       [9.52244103e-01],\n",
       "       [3.03110510e-01],\n",
       "       [1.04178913e-01],\n",
       "       [8.35922182e-01],\n",
       "       [4.08193678e-01],\n",
       "       [4.83735800e-02],\n",
       "       [2.39167102e-02],\n",
       "       [6.21364303e-02],\n",
       "       [9.90971327e-01],\n",
       "       [9.55799222e-01],\n",
       "       [6.57240212e-01],\n",
       "       [8.46193254e-01],\n",
       "       [8.27929974e-01],\n",
       "       [9.96068120e-01],\n",
       "       [8.58720422e-01],\n",
       "       [2.32651472e-01],\n",
       "       [2.51814485e-01],\n",
       "       [8.09824944e-01],\n",
       "       [9.99790490e-01],\n",
       "       [7.80987203e-01],\n",
       "       [3.42539459e-01],\n",
       "       [8.26745033e-01],\n",
       "       [7.54576385e-01],\n",
       "       [9.45699096e-01],\n",
       "       [8.07302296e-02],\n",
       "       [3.42127740e-01],\n",
       "       [5.42473912e-01],\n",
       "       [3.26864034e-01],\n",
       "       [2.59717077e-01],\n",
       "       [1.41369537e-01],\n",
       "       [3.20704579e-01],\n",
       "       [4.65587169e-01],\n",
       "       [2.39999488e-01],\n",
       "       [7.30962586e-03],\n",
       "       [9.61355045e-02],\n",
       "       [7.44424045e-01],\n",
       "       [7.37201214e-01],\n",
       "       [9.60217059e-01],\n",
       "       [3.81430179e-01],\n",
       "       [2.56975554e-02],\n",
       "       [1.98600993e-01],\n",
       "       [3.00254583e-01],\n",
       "       [9.97638941e-01],\n",
       "       [6.03173494e-01],\n",
       "       [8.81594419e-01],\n",
       "       [9.94851410e-01],\n",
       "       [3.86125237e-01],\n",
       "       [9.31203187e-01],\n",
       "       [9.19372857e-01],\n",
       "       [5.57279624e-02],\n",
       "       [5.15789390e-01],\n",
       "       [5.79503119e-01],\n",
       "       [8.23593259e-01],\n",
       "       [3.24421436e-01],\n",
       "       [6.34387553e-01],\n",
       "       [3.38212043e-01],\n",
       "       [6.67656511e-02],\n",
       "       [2.59408891e-01],\n",
       "       [7.91467503e-02],\n",
       "       [2.24222094e-01],\n",
       "       [9.94179726e-01],\n",
       "       [9.99921560e-01],\n",
       "       [6.83353424e-01],\n",
       "       [8.81066501e-01],\n",
       "       [1.37053048e-02],\n",
       "       [7.97604382e-01],\n",
       "       [9.78324234e-01],\n",
       "       [3.27912271e-01],\n",
       "       [5.38817383e-02],\n",
       "       [7.44284689e-01],\n",
       "       [4.01277184e-01],\n",
       "       [9.95465875e-01],\n",
       "       [9.56427991e-01],\n",
       "       [7.51660764e-01],\n",
       "       [3.86689678e-02],\n",
       "       [3.38662937e-02],\n",
       "       [9.84808981e-01],\n",
       "       [9.94263232e-01],\n",
       "       [3.80855173e-01],\n",
       "       [9.03392851e-01],\n",
       "       [9.76790547e-01],\n",
       "       [9.77443993e-01],\n",
       "       [8.31025243e-01],\n",
       "       [6.83156252e-01],\n",
       "       [1.68432340e-01],\n",
       "       [3.94398272e-01],\n",
       "       [8.92599404e-01],\n",
       "       [9.75059211e-01],\n",
       "       [1.19544230e-01],\n",
       "       [9.21405971e-01],\n",
       "       [8.32358062e-01],\n",
       "       [3.63427922e-02],\n",
       "       [2.20718846e-01],\n",
       "       [9.17136550e-01],\n",
       "       [6.81529045e-01],\n",
       "       [4.66944799e-02],\n",
       "       [1.01593472e-01],\n",
       "       [1.12460464e-01],\n",
       "       [6.48017228e-01],\n",
       "       [7.00995505e-01],\n",
       "       [4.80159551e-01],\n",
       "       [9.08471823e-01],\n",
       "       [1.93151653e-01],\n",
       "       [7.56747499e-02],\n",
       "       [8.67965579e-01],\n",
       "       [8.05701733e-01],\n",
       "       [9.98027861e-01],\n",
       "       [2.02797912e-02],\n",
       "       [1.34291664e-01],\n",
       "       [1.27399683e-01],\n",
       "       [6.09937191e-01],\n",
       "       [9.93972957e-01],\n",
       "       [9.18195367e-01],\n",
       "       [4.00196105e-01],\n",
       "       [9.83835578e-01],\n",
       "       [9.13435817e-01],\n",
       "       [9.81457651e-01],\n",
       "       [9.14114952e-01],\n",
       "       [9.41247106e-01],\n",
       "       [7.15703368e-01],\n",
       "       [6.31011546e-01],\n",
       "       [8.97556007e-01],\n",
       "       [1.22011773e-01],\n",
       "       [7.09846854e-01],\n",
       "       [3.41910630e-01],\n",
       "       [9.83834863e-01],\n",
       "       [6.76089153e-02],\n",
       "       [3.97670984e-01],\n",
       "       [3.33637983e-01],\n",
       "       [4.92747247e-01],\n",
       "       [8.22048903e-01],\n",
       "       [8.89971614e-01],\n",
       "       [3.08229595e-01],\n",
       "       [9.73186195e-01],\n",
       "       [8.64530623e-01],\n",
       "       [2.07294039e-02],\n",
       "       [9.31081623e-02],\n",
       "       [7.97487795e-01],\n",
       "       [7.57520497e-01],\n",
       "       [1.23910822e-01],\n",
       "       [9.27203238e-01],\n",
       "       [1.24210902e-01],\n",
       "       [7.99395144e-01],\n",
       "       [8.10586512e-01],\n",
       "       [9.94646966e-01],\n",
       "       [1.28338933e-01],\n",
       "       [6.05625771e-02],\n",
       "       [9.18363750e-01],\n",
       "       [7.50919521e-01],\n",
       "       [9.60593760e-01],\n",
       "       [9.43457067e-01],\n",
       "       [3.23130339e-01],\n",
       "       [4.45958078e-01],\n",
       "       [8.08064640e-01],\n",
       "       [2.61554748e-01],\n",
       "       [9.75843132e-01],\n",
       "       [9.93420184e-01],\n",
       "       [9.81538773e-01],\n",
       "       [9.15949166e-01],\n",
       "       [9.91859019e-01],\n",
       "       [6.56357646e-01],\n",
       "       [9.34375167e-01],\n",
       "       [7.00542033e-01],\n",
       "       [9.85048354e-01],\n",
       "       [8.79382849e-01],\n",
       "       [1.29799575e-01],\n",
       "       [9.24907029e-01],\n",
       "       [2.69189656e-01],\n",
       "       [9.62663352e-01],\n",
       "       [7.14098513e-01],\n",
       "       [9.15895283e-01],\n",
       "       [5.92194557e-01],\n",
       "       [9.72865880e-01],\n",
       "       [3.19842845e-02],\n",
       "       [6.20011725e-02],\n",
       "       [9.88094151e-01],\n",
       "       [1.03357613e-01],\n",
       "       [7.47797847e-01],\n",
       "       [7.26126671e-01],\n",
       "       [9.84051943e-01],\n",
       "       [8.72629941e-01],\n",
       "       [9.96968925e-01],\n",
       "       [5.57448506e-01],\n",
       "       [5.79404831e-01],\n",
       "       [9.92085040e-01],\n",
       "       [9.85735118e-01],\n",
       "       [9.91833925e-01],\n",
       "       [3.61445732e-02],\n",
       "       [4.42172021e-01],\n",
       "       [3.81925136e-01],\n",
       "       [9.52579856e-01],\n",
       "       [2.77922451e-01],\n",
       "       [5.29832244e-01],\n",
       "       [8.88562858e-01],\n",
       "       [7.85479486e-01],\n",
       "       [8.95048380e-01],\n",
       "       [9.20069039e-01],\n",
       "       [9.83336627e-01],\n",
       "       [9.96204168e-02],\n",
       "       [2.76042908e-01],\n",
       "       [4.07288074e-01],\n",
       "       [9.97819662e-01],\n",
       "       [9.85942423e-01],\n",
       "       [8.21818888e-01],\n",
       "       [9.95860279e-01],\n",
       "       [7.32730269e-01],\n",
       "       [8.67296875e-01],\n",
       "       [9.65768039e-01],\n",
       "       [4.22650576e-01],\n",
       "       [2.33081400e-01],\n",
       "       [9.11634505e-01],\n",
       "       [1.45730779e-01],\n",
       "       [2.26135999e-01],\n",
       "       [1.21690623e-01],\n",
       "       [3.76907960e-02],\n",
       "       [7.03795031e-02],\n",
       "       [3.73698711e-01],\n",
       "       [9.62602794e-01],\n",
       "       [7.48880148e-01],\n",
       "       [2.58570641e-01],\n",
       "       [6.27864063e-01],\n",
       "       [5.18088341e-01],\n",
       "       [3.52857113e-02],\n",
       "       [4.74214137e-01],\n",
       "       [4.75188226e-01],\n",
       "       [1.76265076e-01],\n",
       "       [5.31320274e-02],\n",
       "       [2.33304482e-02],\n",
       "       [9.79328334e-01],\n",
       "       [8.18513215e-01],\n",
       "       [9.95458126e-01],\n",
       "       [8.82446110e-01],\n",
       "       [9.84727293e-02],\n",
       "       [9.34514105e-01],\n",
       "       [7.62763560e-01],\n",
       "       [8.60923707e-01],\n",
       "       [3.69149148e-02],\n",
       "       [8.35601270e-01],\n",
       "       [5.69301605e-01],\n",
       "       [6.93721294e-01],\n",
       "       [1.48772702e-01],\n",
       "       [7.20914230e-02],\n",
       "       [9.69452083e-01],\n",
       "       [8.33804011e-02],\n",
       "       [6.61096096e-01],\n",
       "       [7.93172777e-01],\n",
       "       [7.35466957e-01],\n",
       "       [9.73828554e-01],\n",
       "       [9.91294980e-01],\n",
       "       [4.84141856e-02],\n",
       "       [3.11879307e-01],\n",
       "       [9.23076272e-02],\n",
       "       [9.26949024e-01],\n",
       "       [8.31822991e-01],\n",
       "       [4.37878847e-01],\n",
       "       [1.72064621e-02],\n",
       "       [1.34423375e-01],\n",
       "       [9.95691836e-01],\n",
       "       [9.90289867e-01],\n",
       "       [8.41293335e-01],\n",
       "       [1.60277024e-01],\n",
       "       [5.99123359e-01],\n",
       "       [5.19509315e-01],\n",
       "       [1.89463839e-01],\n",
       "       [3.60404015e-01],\n",
       "       [1.67113885e-01],\n",
       "       [6.23653948e-01],\n",
       "       [3.65112424e-02],\n",
       "       [2.73200780e-01],\n",
       "       [4.65968959e-02],\n",
       "       [5.57057023e-01],\n",
       "       [1.61612421e-01],\n",
       "       [8.71024072e-01],\n",
       "       [2.27539957e-01],\n",
       "       [7.74337292e-01],\n",
       "       [8.71598184e-01],\n",
       "       [6.48447275e-01],\n",
       "       [3.92837942e-01],\n",
       "       [8.01483154e-01],\n",
       "       [2.71950275e-01],\n",
       "       [4.96358693e-01],\n",
       "       [9.95342016e-01],\n",
       "       [9.69089687e-01],\n",
       "       [3.09719592e-01],\n",
       "       [7.53280282e-01],\n",
       "       [9.68478680e-01],\n",
       "       [5.06700315e-02],\n",
       "       [2.80480921e-01],\n",
       "       [4.26001132e-01],\n",
       "       [9.25974429e-01],\n",
       "       [2.35716969e-01],\n",
       "       [8.67902756e-01],\n",
       "       [9.75789070e-01],\n",
       "       [9.06158090e-01],\n",
       "       [9.27536011e-01],\n",
       "       [3.10551792e-01],\n",
       "       [8.68214607e-01],\n",
       "       [5.85641377e-02],\n",
       "       [9.86159623e-01]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pred.to('cpu').data.numpy()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, vocab_len):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.emb = nn.Embedding(embedding_dim=emb_dim, num_embeddings=vocab_len)\n",
    "        self.conv = nn.Conv1d(kernel_size=5, out_channels=100, in_channels=emb_dim)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        conv_out = self.conv(emb.transpose(1, 2))\n",
    "        h1 = F.relu(F.max_pool1d(conv_out, conv_out.size(2))).squeeze(2)\n",
    "        return F.sigmoid(self.out(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1 x 4). Kernel size: (1 x 5). Kernel size can't greater than actual input size at /pytorch/aten/src/THNN/generic/SpatialConvolutionMM.c:48",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-33a9ce0dc8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-facbcc04bda8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1 x 4). Kernel size: (1 x 5). Kernel size can't greater than actual input size at /pytorch/aten/src/THNN/generic/SpatialConvolutionMM.c:48"
     ]
    }
   ],
   "source": [
    "mdl = CNNNet(5, len(word2id))\n",
    "mdl(fake_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "0.748\n",
      "0.416\n",
      "0.100\n",
      "0.031\n",
      "0.061\n",
      "0.062\n",
      "0.024\n",
      "0.017\n",
      "0.001\n",
      "0.000\n",
      "16.03 sec\n"
     ]
    }
   ],
   "source": [
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.005)\n",
    "print(mdl)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{:.3f}'.format(running_loss/i))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_valid, df_y_train, df_y_valid = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=42, stratify=df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "7.968\n",
      "5.193\n",
      "3.413\n",
      "2.249\n",
      "1.725\n",
      "1.478\n",
      "1.265\n",
      "1.192\n",
      "0.332\n",
      "0.190\n",
      "46.80 sec\n"
     ]
    }
   ],
   "source": [
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{:.3f}'.format(running_loss/(i%30+1)))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698  0.627 0.627\n",
      "0.454  0.627 0.662\n",
      "0.308  0.627 0.825\n",
      "0.263  0.554 0.554\n",
      "0.175  0.554 0.578\n",
      "0.087  0.547 0.547\n",
      "0.048  0.547 0.578\n",
      "0.036  0.529 0.529\n",
      "0.023  0.529 0.543\n",
      "0.018  0.529 0.537\n",
      "0.015  0.529 0.540\n",
      "20.61 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/i, best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (out): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type AVGNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757  0.813 0.813\n",
      "0.705  0.665 0.665\n",
      "0.566  0.665 0.665\n",
      "0.433  0.604 0.604\n",
      "0.304  0.473 0.473\n",
      "0.191  0.424 0.424\n",
      "0.125  0.402 0.402\n",
      "0.096  0.402 0.408\n",
      "0.057  0.377 0.377\n",
      "0.046  0.377 0.379\n",
      "0.037  0.367 0.367\n",
      "0.035  0.367 0.370\n",
      "0.033  0.367 0.410\n",
      "0.037  0.367 0.492\n",
      "15.91 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = AVGNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.01)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/i, best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('/media/dbonadiman/Data linux/embs/glove.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2009Covidien',\n",
       " '11981',\n",
       " 'Hindsboro',\n",
       " '700m2',\n",
       " 'Lindsanity',\n",
       " 'Streetmate',\n",
       " 'small-diced',\n",
       " '90,000-square-foot',\n",
       " 'Buchannan',\n",
       " 'fair-lending',\n",
       " 'surfjac',\n",
       " 'IZAR',\n",
       " 'KRS1',\n",
       " 'aagain',\n",
       " 'Moriwaki',\n",
       " 'perfromed',\n",
       " 'IIINew',\n",
       " 'Bittenbender',\n",
       " 'out-of-step',\n",
       " 'NataliePace.com',\n",
       " 'NYDailyNews',\n",
       " 'Gsm',\n",
       " 'grabbag',\n",
       " 'ients',\n",
       " 'pic_only',\n",
       " 'Signifyin',\n",
       " 'D7000',\n",
       " 'non-Hindi',\n",
       " 'Centercutting',\n",
       " '16740',\n",
       " 'Buñol',\n",
       " 'Qto',\n",
       " 'Prehosp',\n",
       " 'Creamware',\n",
       " 'Muljat',\n",
       " 'HunterThe',\n",
       " 'Cyniquian',\n",
       " 'Gerbeaud',\n",
       " 'meloncholy',\n",
       " '45oz',\n",
       " 'dfi',\n",
       " 'Shotshell',\n",
       " '18:47:32',\n",
       " '05:07:49',\n",
       " '1,2009',\n",
       " '14-room',\n",
       " 'Henok',\n",
       " 'SpecialsParts',\n",
       " 'member?Register',\n",
       " 'reporter/editor',\n",
       " 'Sopar',\n",
       " 'APOA',\n",
       " 'Lumpkin',\n",
       " 'mannan',\n",
       " 'Habenero',\n",
       " 'CENTRALE',\n",
       " 'schmitty',\n",
       " 'POOLED',\n",
       " 'caved-in',\n",
       " 'LANforge',\n",
       " '12/19/2002',\n",
       " '14:36:09',\n",
       " 'annexing',\n",
       " 'hisses',\n",
       " 'Candaras',\n",
       " 'SAFM',\n",
       " '1994,1995',\n",
       " 'Lintu',\n",
       " 'Montanelli',\n",
       " 'prodige',\n",
       " 'cold-season',\n",
       " '-2266',\n",
       " 'feet-first',\n",
       " 'somethingPoliticslearn',\n",
       " 'Erysimum',\n",
       " 'Z-SLCT',\n",
       " 'lauguage',\n",
       " 'chapesses',\n",
       " 'Sharieff',\n",
       " 'Wilanow',\n",
       " 'nici',\n",
       " 'Hayrettin',\n",
       " 'scintillations',\n",
       " 'Kampsen',\n",
       " 'Intevac',\n",
       " 'interplaying',\n",
       " 'Undesired',\n",
       " 'Reconect',\n",
       " 'Ingela',\n",
       " 'no-asset',\n",
       " 'Schroll',\n",
       " 'Elgaard',\n",
       " 'Scampy',\n",
       " 'Sebra',\n",
       " ':6667',\n",
       " 'MTVu',\n",
       " 'Frilling',\n",
       " 'Oration',\n",
       " '6FP',\n",
       " 'Commas',\n",
       " 'ManagerPosition',\n",
       " 'Durkac',\n",
       " 'Munawwar',\n",
       " 'brevicauda',\n",
       " '21979',\n",
       " 'Y.R.U.',\n",
       " '2,934',\n",
       " 'Gobbos',\n",
       " 'Stewies',\n",
       " 'georgiapeach',\n",
       " 'chemometric',\n",
       " 'G6PD',\n",
       " 'Lekker',\n",
       " 'Modhera',\n",
       " 'landscape.Google',\n",
       " 'Rufo',\n",
       " 'touchingly',\n",
       " 'Norton-Taylor',\n",
       " 'untuk',\n",
       " 'graminifolia',\n",
       " 'Greenhouses',\n",
       " '2-Valve',\n",
       " 'ERISA',\n",
       " 'facelifts',\n",
       " '3/10/2008',\n",
       " 'ServicesComputer',\n",
       " 'Paleos',\n",
       " 'Wuellner',\n",
       " '#thatcamp',\n",
       " 'tiwa',\n",
       " '427-5007',\n",
       " 'interlacings',\n",
       " '7507',\n",
       " 'full-sheet',\n",
       " '31-01',\n",
       " 'ypcat',\n",
       " 'iseebigbooty',\n",
       " '16-11-2009',\n",
       " 'Zorreguieta',\n",
       " 'skiplist',\n",
       " 'Hanapepe',\n",
       " '02:01:36',\n",
       " 'flip-out',\n",
       " 'Saturos',\n",
       " 'Besthorpe',\n",
       " 'critiria',\n",
       " 'techno',\n",
       " 'Pending',\n",
       " 'PlayOffs',\n",
       " 'ScopusZ',\n",
       " 'Bsby',\n",
       " 'videosOther',\n",
       " 'Developpeur',\n",
       " 'Engineer2',\n",
       " 'HighSchoolOT.com',\n",
       " '25,393',\n",
       " 'PMlooking',\n",
       " 'accesskeys',\n",
       " 'ELOCON',\n",
       " 'TicketsWickedWednesday',\n",
       " '10,470',\n",
       " 'Maxim.com',\n",
       " 'Janoah',\n",
       " '14747',\n",
       " 'debris-free',\n",
       " 'Fidjeland',\n",
       " 'Maxia',\n",
       " '19894',\n",
       " 'Burdur',\n",
       " '13:03:19',\n",
       " 'eur/usd',\n",
       " '58/100Platin',\n",
       " 'Bê',\n",
       " 'GeoDataSource',\n",
       " '2008.05.13',\n",
       " ':1757',\n",
       " 'Pico-C',\n",
       " 'unsign',\n",
       " 'Exteriors',\n",
       " 'BIAN',\n",
       " 'Texas.The',\n",
       " 'Monumentos',\n",
       " '130,500',\n",
       " 'Lonfg',\n",
       " 'S.I.D.S.',\n",
       " 'soyjoy',\n",
       " 'WinBench',\n",
       " '14:40:19',\n",
       " 'Bible-NIVHardback',\n",
       " 'acquisition.Need',\n",
       " 'WhoScored.com',\n",
       " 'Dayyan',\n",
       " 'Bertho',\n",
       " 'Comares',\n",
       " 'Reunite',\n",
       " 'PMNs',\n",
       " 'Bigzaggilo',\n",
       " 'Al-Qabas',\n",
       " '@LeadYourTeam',\n",
       " 'termagant',\n",
       " 'Cave',\n",
       " '13-Great',\n",
       " '65-85',\n",
       " '75604',\n",
       " 'curvier',\n",
       " 'ac_status',\n",
       " 'repetitious',\n",
       " 'Abhorsen',\n",
       " '19/09/11',\n",
       " '30/6/2011',\n",
       " 'Muri',\n",
       " 'reclassifying',\n",
       " 'Kirvu',\n",
       " 'fabricantes',\n",
       " 'cedrick',\n",
       " 'EasyAsk',\n",
       " 'gusano',\n",
       " 'older.I',\n",
       " 'SHIELD1',\n",
       " 'Recitals',\n",
       " 'ahev',\n",
       " '.245',\n",
       " '95-118',\n",
       " 'RedTube',\n",
       " 'DTN-X',\n",
       " 'said.To',\n",
       " 'Wakizashi',\n",
       " 'onThink',\n",
       " 'Spinous',\n",
       " '27,482',\n",
       " 'polycyclo',\n",
       " 'lrb',\n",
       " '47665',\n",
       " '3-1-07',\n",
       " '7TWO',\n",
       " 'Enigma-devel',\n",
       " 'Margus',\n",
       " 'Half-an-hour',\n",
       " '2007February',\n",
       " '397.8',\n",
       " 'Forsyth',\n",
       " 'interDesign',\n",
       " 'Oam',\n",
       " 'glowstringing',\n",
       " 'microfinancing',\n",
       " '162.3',\n",
       " 'Black/Purple',\n",
       " 'Venne',\n",
       " 'Skiros',\n",
       " 'yez',\n",
       " 'Percentage0',\n",
       " 'Wyldfyre',\n",
       " 'Baumholder',\n",
       " '05/15/06',\n",
       " 'Role-plays',\n",
       " '14Kt',\n",
       " 'Evaro',\n",
       " 'STAPH',\n",
       " 'whatzit',\n",
       " 'Attacat',\n",
       " 'Kamshet',\n",
       " 'hxg6',\n",
       " 'seventy-odd',\n",
       " '20120301060000',\n",
       " 'children.There',\n",
       " '44707',\n",
       " 'Detektor',\n",
       " '33472',\n",
       " 'Tweed-New',\n",
       " 'up?You',\n",
       " '2013Advanced',\n",
       " 'REMOVALS',\n",
       " '10,993',\n",
       " 'view.More',\n",
       " 'TEOCO',\n",
       " 'CF-52',\n",
       " 'commoditizing',\n",
       " '300-series',\n",
       " 'Dimatteo',\n",
       " '2002/04',\n",
       " 'lagged',\n",
       " 'BSER',\n",
       " 'Performance-enhancing',\n",
       " 'Celex',\n",
       " 'Hahm',\n",
       " '4Sqft',\n",
       " 'Bicolandia',\n",
       " 'buf.append',\n",
       " '13:12:46',\n",
       " 'UserControl',\n",
       " 'slutwalk',\n",
       " '10:45:35',\n",
       " 'TN9',\n",
       " 'LIVERPOOL',\n",
       " 'Sohes',\n",
       " 'Superforecast',\n",
       " 'Ponle',\n",
       " '27,248',\n",
       " 'existente',\n",
       " '17:53:14',\n",
       " '250mcg',\n",
       " '16:42:51',\n",
       " 'brakeforce',\n",
       " '120.48',\n",
       " '23,806',\n",
       " 'daywork',\n",
       " 'fullservice',\n",
       " 'ITH',\n",
       " 'Geotail',\n",
       " 'enterest',\n",
       " '22:29:52',\n",
       " 'estimable',\n",
       " '20:13:29',\n",
       " 'On-boarding',\n",
       " 'postmitotic',\n",
       " 'seaboards',\n",
       " 'Tropheus',\n",
       " 'footballs',\n",
       " 'sacrilicious',\n",
       " 'OVERPRINT',\n",
       " 'MEOH',\n",
       " 'X00',\n",
       " 'cyber-squatting',\n",
       " '7/2004',\n",
       " 'casinoflotholt',\n",
       " 'ssf/3681',\n",
       " 'Yumin',\n",
       " 'Charlottes',\n",
       " 'Co-ordination',\n",
       " 'Datafile',\n",
       " 'lagunitas',\n",
       " '11/04/06',\n",
       " 'Grinny',\n",
       " 'UKDPC',\n",
       " 'scambled',\n",
       " '2,085',\n",
       " 'legendarily',\n",
       " 'EXTRUDED',\n",
       " '15,146',\n",
       " 'sweetlove15',\n",
       " 'extraordianry',\n",
       " 'tons',\n",
       " 'handmaids',\n",
       " '1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950In',\n",
       " 'judices',\n",
       " 'Debut_FinView',\n",
       " 'tunners',\n",
       " 'SurferPower',\n",
       " 'aged',\n",
       " 'Curecanti',\n",
       " 'Veselka',\n",
       " 'GRANJA',\n",
       " 'fuller',\n",
       " 'Chineese',\n",
       " 'Department.More',\n",
       " 'legitimize',\n",
       " 'Brockwell',\n",
       " 'bodies.',\n",
       " 'tetracarboxylic',\n",
       " 'Chachere',\n",
       " 'request.Item',\n",
       " 'Zoologicae',\n",
       " 'fume',\n",
       " 'spic-and-span',\n",
       " 'Torrequebrada',\n",
       " 'Colham',\n",
       " 'SANGAM',\n",
       " 'QuoteComputer',\n",
       " 'pt5',\n",
       " 'Pilchards',\n",
       " 'Cervi',\n",
       " 'Kapin',\n",
       " 'Ducko',\n",
       " '178.43',\n",
       " 'uschar',\n",
       " '22:56:26',\n",
       " 'natural-born',\n",
       " 'mc60',\n",
       " 'Joys',\n",
       " '74931',\n",
       " 'high-definition',\n",
       " 'smushed',\n",
       " '36/37',\n",
       " '233-34',\n",
       " 'Musclebuilding',\n",
       " 'Ex-Mormon',\n",
       " 'J-series',\n",
       " '2010/05/05',\n",
       " 'HOTELREVIEWSReviews',\n",
       " 'piraeus',\n",
       " 'nosie',\n",
       " 'Thiothixene',\n",
       " 'Boilen',\n",
       " 'Sesli',\n",
       " 'UrbanPromise',\n",
       " '163.25',\n",
       " 'Aboubacar',\n",
       " '6/22/01',\n",
       " 'Opem',\n",
       " 'CPANEL',\n",
       " 'vardi',\n",
       " '80535',\n",
       " 'SPZ',\n",
       " '27,297',\n",
       " 'Heliocol',\n",
       " 'setAccount',\n",
       " 'Wallsticker',\n",
       " 'Nawal',\n",
       " 'válasz',\n",
       " 'Deep-Sea',\n",
       " '17313',\n",
       " 'CONDUCTORS',\n",
       " 'basicscasino',\n",
       " 'non-pareils',\n",
       " '455-458',\n",
       " 'smackeroos',\n",
       " 'MARINEMen',\n",
       " '2013Course',\n",
       " 'Moliets',\n",
       " '41-1/4',\n",
       " 'Draker',\n",
       " 'Tioga',\n",
       " 'standard-issue',\n",
       " 'businessA',\n",
       " 'pediacare',\n",
       " 'Kizan',\n",
       " 'Nematodes',\n",
       " 'ritardando',\n",
       " 'yea',\n",
       " 'rail600downtown',\n",
       " 'Ionithermie',\n",
       " 'netteller',\n",
       " 'Gm1',\n",
       " 'Aucas',\n",
       " 'Wine-Spirits',\n",
       " 'autocalibration',\n",
       " 'VersionProduct',\n",
       " 'Glebe',\n",
       " 'Coronacion',\n",
       " 'TypeMagazine',\n",
       " 'ContractsAdd',\n",
       " 'itemssorted',\n",
       " 'lower-density',\n",
       " 'shorstop',\n",
       " 'now.mp3',\n",
       " 'Corniglia',\n",
       " '113-129',\n",
       " '25PK',\n",
       " 'Ambrisentan',\n",
       " '2005-12-12',\n",
       " 'matial',\n",
       " 'Hallan',\n",
       " 'NecklaceLogin',\n",
       " 'MEGANE',\n",
       " 'Sammantha',\n",
       " 'trailrides',\n",
       " '10,900',\n",
       " 'EURoverhead',\n",
       " 'Hollyford',\n",
       " 'T62',\n",
       " 'PORTLANDIA',\n",
       " '13:31:58',\n",
       " '323-24',\n",
       " '178.97',\n",
       " 'Policy1500Check-out',\n",
       " '31-7/8',\n",
       " 'mbit/s',\n",
       " 'Oldcorn',\n",
       " '13025',\n",
       " '46:51',\n",
       " 'OShea',\n",
       " '2/55',\n",
       " '1200T',\n",
       " 'ToolsProfessional',\n",
       " 'hidded',\n",
       " '304KB',\n",
       " 'mousses',\n",
       " 'Mucous',\n",
       " 'mmhhh',\n",
       " 'test_1',\n",
       " 'compatible',\n",
       " 'Athitakis',\n",
       " 'Randels',\n",
       " '.7469',\n",
       " ':5600',\n",
       " 'out-shot',\n",
       " 'Kapital',\n",
       " '3/30/2009',\n",
       " 'caotic',\n",
       " 'dateAdults1234Children01234SubmitGroups',\n",
       " 'Petrolio',\n",
       " '2010.09.29',\n",
       " 'denisebefore',\n",
       " 'K-POP',\n",
       " '450-453',\n",
       " 'Hippogryph',\n",
       " 'WERELSE',\n",
       " 'wondered',\n",
       " 'Lawmaker',\n",
       " 'Skandi',\n",
       " 'articleNatural',\n",
       " 'ToursWMMO',\n",
       " 'amhi',\n",
       " 'monstreux',\n",
       " 'Citlali',\n",
       " 'Neddle',\n",
       " 'P/PA',\n",
       " '03:27:29',\n",
       " '4.94',\n",
       " '5/8/05',\n",
       " 'Gillz',\n",
       " '4,647',\n",
       " 'Ringeisen',\n",
       " 'Burrard-Lucas',\n",
       " 'XSAN',\n",
       " 'xserve',\n",
       " 'Orix',\n",
       " 'a53',\n",
       " 'ViewOverall',\n",
       " 'ParodyHow',\n",
       " 'Zatlers',\n",
       " 'Ulticom',\n",
       " 'Maikon',\n",
       " 'Asesino',\n",
       " 'xx-xx',\n",
       " 'Tarja',\n",
       " 'jupiterclub',\n",
       " 'languageThe',\n",
       " 'stumpings',\n",
       " 'Diposkan',\n",
       " 'Padawan',\n",
       " '227-229',\n",
       " 'Gaiteros',\n",
       " 'Scarywomn',\n",
       " '3568',\n",
       " '1:19:55',\n",
       " '6600i',\n",
       " 'Roelfsema',\n",
       " 'Eversole',\n",
       " 'EpsonNet',\n",
       " 'Bazille',\n",
       " 'Kreder',\n",
       " 'bolita',\n",
       " '10:51:55',\n",
       " 'Hunz',\n",
       " 'Overson',\n",
       " 'krish',\n",
       " '2003-09-01',\n",
       " 'pmOur',\n",
       " 'Commondale',\n",
       " '13:29:09',\n",
       " 'Puppala',\n",
       " 'LUMLEY',\n",
       " 'Alejos',\n",
       " 'D@mn',\n",
       " 'leafminer',\n",
       " 'Tablerock',\n",
       " 'POLAROID',\n",
       " '30-Jul-2004',\n",
       " 'GeronimusClone',\n",
       " 'interactives',\n",
       " 'www.pornaccess',\n",
       " 'Cinturato',\n",
       " 'ngtonWest',\n",
       " 'Rouffiac',\n",
       " 'N’T',\n",
       " '1880-1963',\n",
       " 'dewitt',\n",
       " 'Encompassing',\n",
       " 'Sturniolo',\n",
       " '09-Jan-2011',\n",
       " 'w/timer',\n",
       " 'HomeStudy',\n",
       " 'Scherb',\n",
       " 'Vedant',\n",
       " 'Facepack',\n",
       " 'recruitement',\n",
       " '03-Sep-09',\n",
       " 'this?We',\n",
       " 'yellowing',\n",
       " 'charcoal-colored',\n",
       " 'LongView',\n",
       " 'electron-deficient',\n",
       " '40741',\n",
       " 'unseelie',\n",
       " '2011.03.01',\n",
       " 'fruitage',\n",
       " 'Fußball-Bundesliga',\n",
       " 'Memup',\n",
       " 'benieuwd',\n",
       " '16,549',\n",
       " 'OSE',\n",
       " 'OPTIFADE',\n",
       " 'GPOVes',\n",
       " 'Vagabondia',\n",
       " 'VidaOne',\n",
       " '2013Main',\n",
       " 'Rictus',\n",
       " '04-14-05',\n",
       " '1st-12th',\n",
       " 'endplate',\n",
       " 'resultsBy',\n",
       " 'ResourceLink',\n",
       " 'FANSITE',\n",
       " 'ULINE',\n",
       " 'pre-downloaded',\n",
       " 'Saunder',\n",
       " '.8879',\n",
       " 'Musclemag',\n",
       " 'read_mostly',\n",
       " 'tutoriel',\n",
       " 'poesie',\n",
       " 'Muruch',\n",
       " 'AtheroGenics',\n",
       " '20acknowledges',\n",
       " '44110',\n",
       " '19:58:20',\n",
       " '81242',\n",
       " '23:23:59',\n",
       " 'ha3',\n",
       " '37,770',\n",
       " 'snoo',\n",
       " '14:03:14',\n",
       " 'Full-Range',\n",
       " '03:41:09',\n",
       " 'correlator',\n",
       " '13586',\n",
       " '11-of-12',\n",
       " 'orwellian',\n",
       " 'assesories',\n",
       " 'Frattin',\n",
       " '29-Nov-2005',\n",
       " 'blacksite',\n",
       " 'Numico',\n",
       " 'sighhhhh',\n",
       " 'soluces',\n",
       " 'gnp_url',\n",
       " 'Fyrd',\n",
       " 'MOULDINGS',\n",
       " 'tribal-state',\n",
       " 'shittier',\n",
       " 'thoughI',\n",
       " 'Pan-India',\n",
       " 'Speed-Connect',\n",
       " 'Savannakhet',\n",
       " 'irrupt',\n",
       " 'Neeltje',\n",
       " '2004/05/13',\n",
       " 'Crissie',\n",
       " 'BCP1',\n",
       " 'Need-to-know',\n",
       " 'Schützen',\n",
       " 'Sun-day',\n",
       " '47920',\n",
       " 'CLAREY',\n",
       " 'Yorba',\n",
       " 'Draggin',\n",
       " 'dissorder',\n",
       " 'andrology',\n",
       " '102.10',\n",
       " 'WDFNL',\n",
       " 'LMAOOOOOOOO',\n",
       " 'vegito',\n",
       " 'search_results',\n",
       " 'Tegatech',\n",
       " '439,000',\n",
       " 'Electronica/Dance',\n",
       " 'Kölbl',\n",
       " 'Valera',\n",
       " 'Alky',\n",
       " 'catolica',\n",
       " '165.6',\n",
       " '1237power',\n",
       " 'Matijevic',\n",
       " '03.18.12',\n",
       " 'inshell',\n",
       " 'MMTC',\n",
       " 'Blasius',\n",
       " 'AVEDON',\n",
       " 'RADIOACTIVITY',\n",
       " 'Ruah',\n",
       " 'one-for-all',\n",
       " '94588',\n",
       " 'May-17-06',\n",
       " '10:27:22',\n",
       " '#silly',\n",
       " 'Durlin',\n",
       " 'Lunney',\n",
       " 'Bellaonline',\n",
       " '1,96',\n",
       " 'maintaning',\n",
       " 'Jandra',\n",
       " 'V-Flex',\n",
       " '2/1/2001',\n",
       " '31-May-09',\n",
       " 'aspx?PowerPointView',\n",
       " '5:09',\n",
       " 'points-11',\n",
       " 'Wattenmeer',\n",
       " 'godine',\n",
       " '7-10th',\n",
       " '23:28:22',\n",
       " '300x300mm',\n",
       " 'ALT',\n",
       " 'Enormity',\n",
       " 'redirect_uri',\n",
       " 'ASFPM',\n",
       " 'Graying',\n",
       " 'December-February',\n",
       " '478.53750',\n",
       " 'nonsteroidal',\n",
       " 'AlphabetFilter',\n",
       " 'whoppers',\n",
       " 'Sentamu',\n",
       " 'Yobs',\n",
       " '42133',\n",
       " 'EnglishYear',\n",
       " 'Mind-control',\n",
       " 'floss',\n",
       " 'vimse',\n",
       " 'PROTESS',\n",
       " '1:07:55',\n",
       " '16:47:52',\n",
       " 'DecoratingFrequency',\n",
       " '34474',\n",
       " 'muslim',\n",
       " 'Ungirthed',\n",
       " '43XX',\n",
       " '5,595.00',\n",
       " 'clyster',\n",
       " '20:04:29',\n",
       " 'HeadlinesTrendsCompetitive',\n",
       " 'red/blue/yellow',\n",
       " 'ıt',\n",
       " 'Toomua',\n",
       " 'Sukhoi',\n",
       " 'paysite',\n",
       " '169-186',\n",
       " 'NH&S',\n",
       " 'ergoline',\n",
       " 'Brattan',\n",
       " 'HARKEY',\n",
       " 'FETP',\n",
       " 'WebNX',\n",
       " '19934',\n",
       " 'Kirtana',\n",
       " 'Small-time',\n",
       " 'automobilemag',\n",
       " 'cancers',\n",
       " '26:16',\n",
       " 'divebomb',\n",
       " 'InformationTitle',\n",
       " \"l'examen\",\n",
       " 'TVCatchup',\n",
       " 'Age-Fighting',\n",
       " 'someoen',\n",
       " 'AVADA',\n",
       " 'ausev',\n",
       " 'copying/moving',\n",
       " 'userinit.exe',\n",
       " 'RODAN',\n",
       " 'bombs',\n",
       " 'KKKKKKKKKKKKKKKKKKKKKKKKKKKKKK',\n",
       " 'eight-byte',\n",
       " '16:38:31',\n",
       " 'farmhouse-style',\n",
       " 'tabs/windows',\n",
       " 'DateTime.pmDAG',\n",
       " 'InterCall',\n",
       " 'ticklish',\n",
       " '950ml',\n",
       " '63028',\n",
       " '30MB/s',\n",
       " 'MareMare',\n",
       " 'review4/13/2013',\n",
       " 'burgling',\n",
       " 'Plez',\n",
       " 'MIMM',\n",
       " '9PE',\n",
       " 'Dongarra',\n",
       " '.704',\n",
       " 'Biedenkopf',\n",
       " '03:03:17',\n",
       " 'Kumho',\n",
       " 'ultra-easy',\n",
       " 'gas/propane',\n",
       " 'LACROIX',\n",
       " 'Residenz-Hotel',\n",
       " 'RichWP',\n",
       " 'holld',\n",
       " '1640s',\n",
       " 'Boyett-Compo',\n",
       " '-1016',\n",
       " 'Coslett',\n",
       " 'awards-show',\n",
       " 'Proso',\n",
       " 'cpu_relax',\n",
       " '18:19:48',\n",
       " 'tenne',\n",
       " 'Shewey',\n",
       " 'get_the_category',\n",
       " 'Yaknow',\n",
       " 'Matavz',\n",
       " 'CroZg',\n",
       " 'Sparkletts',\n",
       " 'Assuck',\n",
       " 'GARTEN',\n",
       " 'RECORD',\n",
       " '01477',\n",
       " 'GOLD/SILVER',\n",
       " 'SHERYL',\n",
       " 'room/home',\n",
       " '78019',\n",
       " '12/27/2002',\n",
       " 'DROP-OFF',\n",
       " 'java.awt.Dimension',\n",
       " 'swingeruk',\n",
       " 'Horbling',\n",
       " 'Wellens',\n",
       " 'multi-screen',\n",
       " 'california.html',\n",
       " 'snowday',\n",
       " 'FootHills',\n",
       " '17:53:57',\n",
       " '#sheer',\n",
       " 'douchebags',\n",
       " 'Ryuuji',\n",
       " 'installation/upgrade',\n",
       " '472.50',\n",
       " 'Hoardings',\n",
       " '07:49:26',\n",
       " '#lmao',\n",
       " 'doom-and-gloom',\n",
       " '17:22:42',\n",
       " 'maraca',\n",
       " 'Yavana',\n",
       " 'PreciseAir',\n",
       " 'particle-laden',\n",
       " '&Ndash;',\n",
       " 'Svavar',\n",
       " 'swats',\n",
       " 'Hybla',\n",
       " 'Nishi',\n",
       " '17-27',\n",
       " 'Burghound',\n",
       " 'Bushmanland',\n",
       " 'VIDEOS0',\n",
       " 'Hisaki',\n",
       " 'Metalingus',\n",
       " 'Sztuka',\n",
       " 'Potsgrove',\n",
       " 'vileness',\n",
       " 'layo',\n",
       " '12-28-10',\n",
       " 'NFORCE',\n",
       " 'batholith',\n",
       " 'Tsc2',\n",
       " 'mutilate',\n",
       " 'Gub',\n",
       " 'Touko',\n",
       " '51d',\n",
       " 'dSLR',\n",
       " 'Glorifying',\n",
       " '3984282232',\n",
       " '12:57:54',\n",
       " 'Worrest',\n",
       " 'Inc.Light',\n",
       " 'unthanked',\n",
       " '690m',\n",
       " 'maroccan',\n",
       " 'Royal-Pedic',\n",
       " 'MEDIATOR',\n",
       " 'owner-operated',\n",
       " 'applikationer',\n",
       " 'Infomedia',\n",
       " 'Fiorita',\n",
       " 'Oil-For-Food',\n",
       " 'gyus',\n",
       " 'Chiloe',\n",
       " '2011-10-27',\n",
       " ':42:47',\n",
       " 'accounced',\n",
       " 'MRT80',\n",
       " '04:43:45',\n",
       " 'Aulaqi',\n",
       " 'astrong',\n",
       " 'Sawyer',\n",
       " 'adjoined',\n",
       " 'expiredConfirm',\n",
       " 'Nagravision',\n",
       " '18:37:38',\n",
       " 'alone-ness',\n",
       " 'F-Su',\n",
       " 'Srivilliputhur',\n",
       " 'Instance',\n",
       " '23:44:10',\n",
       " 'dual-driver',\n",
       " 'config.inc.php',\n",
       " 'Veoh',\n",
       " '2-chlorophenyl',\n",
       " 'facundo',\n",
       " '2:40:00',\n",
       " '43.0',\n",
       " '876956',\n",
       " 'Swan-Ganz',\n",
       " 'JAK1',\n",
       " 'stoles',\n",
       " 'Curnock',\n",
       " '59S',\n",
       " 'LYMON',\n",
       " 'dough-like',\n",
       " 'DL13',\n",
       " 'URCC',\n",
       " 'Ohern',\n",
       " 'Isai',\n",
       " 'UOIT',\n",
       " 'BEENLEIGH',\n",
       " '09:20:48',\n",
       " 'BeatMaker',\n",
       " 'RolloverSort',\n",
       " 'CabinetParts.com',\n",
       " '08:16:27',\n",
       " 'Slator',\n",
       " '22:49:48',\n",
       " 'post-1994',\n",
       " 'Winterfell',\n",
       " 'interesting.As',\n",
       " 'X-band',\n",
       " 'rbala',\n",
       " '03:58:49',\n",
       " 'Lcothed',\n",
       " '22:41:37',\n",
       " 'Triton540i',\n",
       " 'Viollent',\n",
       " 'points328',\n",
       " '01:57:23',\n",
       " 'digusting',\n",
       " 'Gabrielian',\n",
       " 'attainder',\n",
       " 'whooping-cough',\n",
       " 'land-tax',\n",
       " 'Worldly',\n",
       " 'carian',\n",
       " 'Premarital',\n",
       " 'Randie',\n",
       " 'p163',\n",
       " '02:36:03',\n",
       " 'Makhaya',\n",
       " 'record-breaking',\n",
       " 'Medallion',\n",
       " 'everdeen',\n",
       " 'PoliticsHow',\n",
       " '3plus',\n",
       " '17:16:34',\n",
       " 'Tahe',\n",
       " 'Ztek',\n",
       " 'TCT',\n",
       " '74153',\n",
       " 'Prills',\n",
       " '12:08:18',\n",
       " 'Omanu',\n",
       " '17:38:04',\n",
       " '14896',\n",
       " 'use.Note',\n",
       " 'Peiling',\n",
       " 'adog',\n",
       " 'interactionapplication',\n",
       " '23,409',\n",
       " 'Molokov',\n",
       " '141,500',\n",
       " '23:08:52',\n",
       " 'Mccleary',\n",
       " 'passing',\n",
       " 'burkhardt',\n",
       " 'Movoie',\n",
       " '4422',\n",
       " 'straponed',\n",
       " 'NT4/2000/2003',\n",
       " 'non-DC',\n",
       " 'Fq',\n",
       " '15lb',\n",
       " '06:43:04',\n",
       " 'Paparoa',\n",
       " '38679',\n",
       " 'queek',\n",
       " '00:42:43',\n",
       " 'sub-lease',\n",
       " 'Murcia',\n",
       " 'Milagros',\n",
       " 'KBase',\n",
       " 'out?If',\n",
       " 'Societal',\n",
       " 'suchasadaffair',\n",
       " 'Watsons',\n",
       " 'Palmer7',\n",
       " '18:37:36',\n",
       " '28,264',\n",
       " 'StreetOmaha',\n",
       " '18:49:58',\n",
       " 'beadboard',\n",
       " ...}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w2v.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df, w2vec=None):\n",
    "    vocab = set()\n",
    "    for doc in df['text']:\n",
    "        vocab |= set(doc.split())\n",
    "    if w2vec:\n",
    "        vocab &= set(w2vec)\n",
    "    return vocab\n",
    "\n",
    "r_vocab =  get_vocab(dataset, w2v.vocab)\n",
    "r_word2id = create_w2id(r_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36521"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['X_r'] = dataset['text'].apply(lambda x :doc2ids(x, r_word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "      <th>X_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes a movie comes along that falls somew...</td>\n",
       "      <td>1</td>\n",
       "      <td>[18709, 6119, 4976, 19592, 13979, 21782, 15443...</td>\n",
       "      <td>[26927, 2808, 20503, 27313, 24671, 9965, 7061,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i swear i have seen the edge before in fact it...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1943, 16863, 1943, 14122, 8610, 10039, 37531,...</td>\n",
       "      <td>[871, 26057, 871, 6489, 3966, 4611, 35554, 194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with the abundance of trite recycled movies in...</td>\n",
       "      <td>1</td>\n",
       "      <td>[4362, 10039, 29077, 28356, 20400, 19701, 6340...</td>\n",
       "      <td>[20238, 4611, 31679, 31335, 27663, 27362, 2110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contact is a film that tries to do several dif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[38731, 21804, 6119, 6444, 21782, 22489, 3285,...</td>\n",
       "      <td>[36085, 9976, 2808, 21147, 9965, 28641, 19723,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expectation rating a bit worse than expected m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[14067, 37634, 6119, 22692, 29894, 35667, 4642...</td>\n",
       "      <td>[24711, 35601, 2808, 28736, 32077, 16306, 2035...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  sometimes a movie comes along that falls somew...      1   \n",
       "1  i swear i have seen the edge before in fact it...      1   \n",
       "2  with the abundance of trite recycled movies in...      1   \n",
       "3  contact is a film that tries to do several dif...      1   \n",
       "4  expectation rating a bit worse than expected m...      1   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [18709, 6119, 4976, 19592, 13979, 21782, 15443...   \n",
       "1  [1943, 16863, 1943, 14122, 8610, 10039, 37531,...   \n",
       "2  [4362, 10039, 29077, 28356, 20400, 19701, 6340...   \n",
       "3  [38731, 21804, 6119, 6444, 21782, 22489, 3285,...   \n",
       "4  [14067, 37634, 6119, 22692, 29894, 35667, 4642...   \n",
       "\n",
       "                                                 X_r  \n",
       "0  [26927, 2808, 20503, 27313, 24671, 9965, 7061,...  \n",
       "1  [871, 26057, 871, 6489, 3966, 4611, 35554, 194...  \n",
       "2  [20238, 4611, 31679, 31335, 27663, 27362, 2110...  \n",
       "3  [36085, 9976, 2808, 21147, 9965, 28641, 19723,...  \n",
       "4  [24711, 35601, 2808, 28736, 32077, 16306, 2035...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(dataset['X_r'], dataset['label'], test_size=0.2, random_state=42, stratify=dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_valid, df_y_train, df_y_valid = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=42, stratify=df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708  0.644 0.644\n",
      "0.472  0.644 0.712\n",
      "0.329  0.644 0.744\n",
      "0.243  0.588 0.588\n",
      "0.147  0.561 0.561\n",
      "0.077  0.554 0.554\n",
      "0.049  0.554 0.580\n",
      "0.038  0.554 0.558\n",
      "0.027  0.554 0.578\n",
      "17.04 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/i, best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.696875"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embs(embs, w2v, dictionary):\n",
    "    e = np.array([w2v[w] for w in w2v.vocab])\n",
    "    embeddings_mean = float(np.mean(e))\n",
    "    embeddings_std = float(np.std(e))\n",
    "    embs.weight.data.normal_(embeddings_mean, embeddings_std)\n",
    "    for w in dictionary:\n",
    "        embs.weight.data[dictionary[w]].copy_(\n",
    "            torch.from_numpy(w2v[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698  0.643 0.643\n",
      "0.538  0.567 0.567\n",
      "0.364  0.485 0.485\n",
      "0.199  0.424 0.424\n",
      "0.095  0.393 0.393\n",
      "0.050  0.393 0.408\n",
      "0.032  0.393 0.405\n",
      "0.021  0.393 0.396\n",
      "0.011  0.380 0.380\n",
      "0.008  0.379 0.379\n",
      "0.006  0.378 0.378\n",
      "0.005  0.378 0.379\n",
      "0.004  0.378 0.382\n",
      "0.003  0.378 0.386\n",
      "25.79 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "initialize_embs(mdl.emb, w2v, r_word2id)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/i, best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80625"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (out): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type AVGNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.617  0.670 0.670\n",
      "2.292  0.644 0.644\n",
      "1.658  0.575 0.575\n",
      "1.036  0.495 0.495\n",
      "0.620  0.397 0.397\n",
      "0.350  0.397 0.406\n",
      "0.307  0.397 0.411\n",
      "0.146  0.366 0.366\n",
      "0.129  0.364 0.364\n",
      "0.108  0.364 0.389\n",
      "0.119  0.364 0.453\n",
      "0.123  0.364 0.592\n",
      "13.84 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = AVGNet(300, len(word2id)).to(device)\n",
    "initialize_embs(mdl.emb, w2v, r_word2id)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=0.01)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8575"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859375"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVGNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (out): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "3.878  0.920 0.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type AVGNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.053  0.674 0.674\n",
      "2.647  0.658 0.658\n",
      "2.566  0.646 0.646\n",
      "2.493  0.636 0.636\n",
      "2.424  0.632 0.632\n",
      "2.374  0.627 0.627\n",
      "2.333  0.622 0.622\n",
      "2.297  0.617 0.617\n",
      "2.265  0.611 0.611\n",
      "2.235  0.606 0.606\n",
      "2.208  0.601 0.601\n",
      "2.182  0.596 0.596\n",
      "2.159  0.592 0.592\n",
      "2.137  0.588 0.588\n",
      "2.117  0.584 0.584\n",
      "2.097  0.580 0.580\n",
      "2.079  0.576 0.576\n",
      "2.062  0.573 0.573\n",
      "2.047  0.569 0.569\n",
      "2.032  0.566 0.566\n",
      "2.017  0.563 0.563\n",
      "2.004  0.560 0.560\n",
      "1.991  0.557 0.557\n",
      "1.979  0.555 0.555\n",
      "1.967  0.552 0.552\n",
      "1.956  0.549 0.549\n",
      "1.946  0.547 0.547\n",
      "1.935  0.545 0.545\n",
      "1.926  0.542 0.542\n",
      "1.916  0.540 0.540\n",
      "1.907  0.538 0.538\n",
      "1.899  0.536 0.536\n",
      "1.890  0.534 0.534\n",
      "1.882  0.532 0.532\n",
      "1.875  0.530 0.530\n",
      "1.867  0.528 0.528\n",
      "1.860  0.526 0.526\n",
      "1.853  0.524 0.524\n",
      "1.846  0.522 0.522\n",
      "1.839  0.521 0.521\n",
      "1.833  0.519 0.519\n",
      "1.827  0.517 0.517\n",
      "1.820  0.516 0.516\n",
      "1.815  0.514 0.514\n",
      "1.809  0.512 0.512\n",
      "1.803  0.511 0.511\n",
      "1.798  0.509 0.509\n",
      "1.792  0.508 0.508\n",
      "1.787  0.507 0.507\n",
      "13.26 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = AVGNet(300, len(word2id)).to(device)\n",
    "initialize_embs(mdl.emb, w2v, r_word2id)\n",
    "mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.1)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734375"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(39698, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.542  0.645 0.645\n",
      "2.043  0.581 0.581\n",
      "1.562  0.517 0.517\n",
      "1.087  0.474 0.474\n",
      "0.714  0.432 0.432\n",
      "0.455  0.390 0.390\n",
      "0.297  0.374 0.374\n",
      "0.211  0.366 0.366\n",
      "0.154  0.364 0.364\n",
      "0.123  0.364 0.418\n",
      "0.113  0.364 0.471\n",
      "0.107  0.364 0.452\n",
      "0.100  0.359 0.359\n",
      "0.086  0.359 0.448\n",
      "0.054  0.359 0.483\n",
      "0.030  0.359 0.362\n",
      "10.97 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, len(word2id)).to(device)\n",
    "initialize_embs(mdl.emb, w2v, r_word2id)\n",
    "mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "val_y = torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1)).to(device)\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.to(device))\n",
    "        loss = criterion(out, y.to(device))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.item()))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.LongTensor(pad_batch(df_X_test)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.LongTensor(pad_batch(df_X_valid)).to(device)\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.to('cpu').data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, vocab):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.emb = nn.Embedding(embedding_dim=emb_dim, num_embeddings=len(vocab),padding_idx=0)\n",
    "        self.conv = nn.Conv1d(kernel_size=5, out_channels=100, in_channels=300)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        conv_out = self.conv(emb.transpose(1, 2))\n",
    "        h1 = F.relu(F.max_pool1d(conv_out, conv_out.size(2))).squeeze(2)\n",
    "        return F.sigmoid(self.out(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(40259, 300, padding_idx=0)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.566  0.658 0.658\n",
      "2.122  0.604 0.604\n",
      "1.673  0.540 0.540\n",
      "1.202  0.486 0.486\n",
      "0.810  0.436 0.436\n",
      "0.531  0.393 0.393\n",
      "0.350  0.372 0.372\n",
      "0.245  0.365 0.365\n",
      "0.174  0.354 0.354\n",
      "0.132  0.354 0.366\n",
      "0.112  0.354 0.400\n",
      "0.099  0.354 0.407\n",
      "10.44 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, w2idx).cuda()\n",
    "initialize_embs(mdl.emb, w2v, restrict_w2id)\n",
    "mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = Variable(torch.LongTensor(pad_batch(df_X_valid))).cuda()\n",
    "val_y = Variable(torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1))).cuda()\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.cuda())\n",
    "        loss = criterion(out, y.cuda())\n",
    "        #if i%30 == 29:\n",
    "        #    print('{:.3f}'.format(running_loss/30))\n",
    "        #    running_loss = 0\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.data[0] < best_val_loss:\n",
    "        best_val_loss = val_loss.data[0]\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.data[0]))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = Variable(torch.LongTensor(pad_batch(df_X_test))).cuda()\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834375"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = Variable(torch.LongTensor(pad_batch(df_X_valid))).cuda()\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(valid_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, vocab):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.emb = nn.Embedding(embedding_dim=emb_dim, num_embeddings=len(vocab),padding_idx=0)\n",
    "        self.conv = nn.Conv1d(kernel_size=5, out_channels=100, in_channels=300)\n",
    "        self.hidd = nn.Linear(100, 100)\n",
    "        self.out = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        conv_out = self.conv(emb.transpose(1, 2))\n",
    "        h1 = F.relu(F.max_pool1d(conv_out, conv_out.size(2))).squeeze(2)\n",
    "        return F.sigmoid(self.out(F.relu(self.hidd(h1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(40259, 300, padding_idx=0)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (hidd): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.587  0.692 0.692\n",
      "2.544  0.691 0.691\n",
      "2.506  0.689 0.689\n",
      "2.461  0.687 0.687\n",
      "2.403  0.684 0.684\n",
      "2.330  0.680 0.680\n",
      "2.236  0.675 0.675\n",
      "2.109  0.668 0.668\n",
      "1.945  0.658 0.658\n",
      "1.740  0.646 0.646\n",
      "1.503  0.630 0.630\n",
      "1.253  0.614 0.614\n",
      "1.012  0.598 0.598\n",
      "0.797  0.583 0.583\n",
      "0.617  0.569 0.569\n",
      "0.475  0.558 0.558\n",
      "0.367  0.548 0.548\n",
      "0.286  0.541 0.541\n",
      "0.226  0.534 0.534\n",
      "0.181  0.529 0.529\n",
      "0.148  0.524 0.524\n",
      "0.122  0.519 0.519\n",
      "0.102  0.515 0.515\n",
      "0.087  0.512 0.512\n",
      "0.075  0.509 0.509\n",
      "0.065  0.507 0.507\n",
      "0.056  0.506 0.506\n",
      "0.050  0.506 0.506\n",
      "0.044  0.505 0.505\n",
      "0.039  0.505 0.505\n",
      "0.035  0.504 0.504\n",
      "0.031  0.503 0.503\n",
      "0.028  0.502 0.502\n",
      "0.025  0.501 0.501\n",
      "0.023  0.500 0.500\n",
      "0.020  0.500 0.500\n",
      "0.019  0.499 0.499\n",
      "0.017  0.499 0.499\n",
      "0.015  0.498 0.498\n",
      "0.014  0.498 0.498\n",
      "0.013  0.498 0.498\n",
      "0.012  0.498 0.498\n",
      "0.011  0.497 0.497\n",
      "0.010  0.497 0.497\n",
      "0.010  0.497 0.497\n",
      "0.009  0.497 0.497\n",
      "0.008  0.497 0.498\n",
      "62.06 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, w2idx).cuda()\n",
    "initialize_embs(mdl.emb, w2v, restrict_w2id)\n",
    "# mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.0001)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = Variable(torch.LongTensor(pad_batch(df_X_valid))).cuda()\n",
    "val_y = Variable(torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1))).cuda()\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.cuda())\n",
    "        loss = criterion(out, y.cuda())\n",
    "        #if i%30 == 29:\n",
    "        #    print('{:.3f}'.format(running_loss/30))\n",
    "        #    running_loss = 0\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.data[0] < best_val_loss:\n",
    "        best_val_loss = val_loss.data[0]\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.data[0]))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = Variable(torch.LongTensor(pad_batch(df_X_test))).cuda()\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (emb): Embedding(40259, 300, padding_idx=0)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  (hidd): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbonadiman/anaconda3/envs/anlp/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CNNNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.587  0.692 0.692\n",
      "2.550  0.691 0.691\n",
      "2.521  0.689 0.689\n",
      "2.487  0.688 0.688\n",
      "2.448  0.685 0.685\n",
      "2.400  0.682 0.682\n",
      "2.342  0.679 0.679\n",
      "2.270  0.673 0.673\n",
      "2.178  0.667 0.667\n",
      "2.061  0.658 0.658\n",
      "1.915  0.646 0.646\n",
      "1.742  0.631 0.631\n",
      "1.550  0.615 0.615\n",
      "1.347  0.598 0.598\n",
      "1.146  0.581 0.581\n",
      "0.959  0.565 0.565\n",
      "0.794  0.551 0.551\n",
      "0.654  0.539 0.539\n",
      "0.539  0.529 0.529\n",
      "0.445  0.521 0.521\n",
      "0.370  0.515 0.515\n",
      "0.310  0.509 0.509\n",
      "0.263  0.505 0.505\n",
      "0.226  0.501 0.501\n",
      "0.195  0.499 0.499\n",
      "0.171  0.496 0.496\n",
      "0.151  0.494 0.494\n",
      "0.135  0.492 0.492\n",
      "0.122  0.491 0.491\n",
      "0.111  0.489 0.489\n",
      "0.101  0.488 0.488\n",
      "0.093  0.486 0.486\n",
      "0.086  0.485 0.485\n",
      "0.080  0.484 0.484\n",
      "0.075  0.483 0.483\n",
      "0.070  0.483 0.483\n",
      "0.066  0.482 0.482\n",
      "0.062  0.482 0.482\n",
      "0.059  0.481 0.481\n",
      "0.056  0.481 0.481\n",
      "0.054  0.481 0.481\n",
      "0.051  0.481 0.481\n",
      "0.048  0.481 0.481\n",
      "0.046  0.481 0.481\n",
      "0.044  0.481 0.482\n",
      "61.07 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, w2idx)\n",
    "initialize_embs(mdl.emb, w2v, restrict_w2id)\n",
    "mdl = mdl.cuda()\n",
    "# mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.0001, weight_decay=0.0005)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = Variable(torch.LongTensor(pad_batch(df_X_valid))).cuda()\n",
    "val_y = Variable(torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1))).cuda()\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.cuda())\n",
    "        loss = criterion(out, y.cuda())\n",
    "        #if i%30 == 29:\n",
    "        #    print('{:.3f}'.format(running_loss/30))\n",
    "        #    running_loss = 0\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.data[0] < best_val_loss:\n",
    "        best_val_loss = val_loss.data[0]\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.data[0]))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = Variable(torch.LongTensor(pad_batch(df_X_test))).cuda()\n",
    "mdl = torch.load('best.pt')\n",
    "pred = mdl(test_X)\n",
    "out = pred.cpu().data.numpy()\n",
    "accuracy_score(y_pred=(out >= 0.5), y_true=df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mdl\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dataset', 34740190),\n",
       " ('df_X_train', 8636936),\n",
       " ('df_X_test', 2634072),\n",
       " ('df_X_valid', 2096896),\n",
       " ('restrict_w2id', 1310816),\n",
       " ('w2idx', 1310816),\n",
       " ('df_y_test', 26904),\n",
       " ('df_y_train', 20504),\n",
       " ('df_y_valid', 15384),\n",
       " ('negative_reviews', 9024),\n",
       " ('positive_reviews', 9024),\n",
       " ('parsed_neg', 8544),\n",
       " ('parsed_pos', 8544),\n",
       " ('AVGNet', 1184),\n",
       " ('CNNNet', 1184),\n",
       " ('KeyedVectors', 1056),\n",
       " ('Variable', 1056),\n",
       " ('accuracy_score', 136),\n",
       " ('batched_dataset', 136),\n",
       " ('get_dataframe', 136),\n",
       " ('initialize_embs', 136),\n",
       " ('load_all', 136),\n",
       " ('lowercased', 136),\n",
       " ('pad_batch', 136),\n",
       " ('r_word2id', 136),\n",
       " ('train_test_split', 136),\n",
       " ('vocabulary', 136),\n",
       " ('word2id', 136),\n",
       " ('out', 112),\n",
       " ('F', 80),\n",
       " ('fake_sent', 80),\n",
       " ('loss', 80),\n",
       " ('nn', 80),\n",
       " ('np', 80),\n",
       " ('pred', 80),\n",
       " ('test_X', 80),\n",
       " ('val_X', 80),\n",
       " ('val_loss', 80),\n",
       " ('val_y', 80),\n",
       " ('valid_X', 80),\n",
       " ('x', 80),\n",
       " ('y', 80),\n",
       " ('sent', 69),\n",
       " ('criterion', 56),\n",
       " ('nlp', 56),\n",
       " ('optimizer', 56),\n",
       " ('w2v', 56),\n",
       " ('epoch', 28),\n",
       " ('i', 28),\n",
       " ('patience', 28),\n",
       " ('wait', 28),\n",
       " ('best_val_loss', 24),\n",
       " ('running_loss', 24),\n",
       " ('start', 24)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "mdl = CNNNet(300, w2idx)\n",
    "initialize_embs(mdl.emb, w2v, restrict_w2id)\n",
    "mdl = mdl.cuda()\n",
    "mdl.emb.weight.requires_grad = False\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, mdl.parameters()), lr=0.0001, weight_decay=0.0005)\n",
    "print(mdl)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "val_X = Variable(torch.LongTensor(pad_batch(df_X_valid))).cuda()\n",
    "val_y = Variable(torch.FloatTensor(np.expand_dims(df_y_valid.as_matrix(),1))).cuda()\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    running_loss = 0\n",
    "    for i,(x, y) in enumerate(batched_dataset(df_X_train, df_y_train)):\n",
    "        optimizer.zero_grad()\n",
    "        out = mdl(x.cuda())\n",
    "        loss = criterion(out, y.cuda())\n",
    "        #if i%30 == 29:\n",
    "        #    print('{:.3f}'.format(running_loss/30))\n",
    "        #    running_loss = 0\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = criterion(mdl(val_X), val_y)\n",
    "    if val_loss.data[0] < best_val_loss:\n",
    "        best_val_loss = val_loss.data[0]\n",
    "        torch.save(mdl, 'best.pt')\n",
    "        wait  = 0\n",
    "    elif wait > patience:\n",
    "        break\n",
    "    else:\n",
    "        wait+=1\n",
    "        \n",
    "    print('{:.3f}  {:.3f} {:.3f}'.format(running_loss/(i%30+1), best_val_loss, val_loss.data[0]))\n",
    "print('{:.2f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
